{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "p_processed=pickle.load(open('../prepared-data/p_processed', 'rb'))\n",
    "n_processed=pickle.load(open('../prepared-data/n_processed', 'rb'))\n",
    "vocabulary=pickle.load(open('../prepared-data/vocabulary', 'rb'))\n",
    "\n",
    "#POS subjective vs objective score (higher means more subjective) added 1 to each\n",
    "tag_scores={'NPS':1-0.82,'WP$':1-0.7,'POS':1-0.45,'NP':1-0.39,'NNS':1-0.32,'IN':1-0.21,'VBN':1-0.21,\n",
    "   'VBZ':1-0.15,'JJR':1-0.1,'RBS':1-0.03,'TO':1+0.01,'NN':1+0.02,'JJS':1+0.05,'JJ':1+0.1,\n",
    "   'DT':1+0.11,'VBG':1+0.15,'VBD':1+0.19,'WDT':1+0.21,'WP':1+0.23,'RP':1+0.24,'MD':1+0.3,'VB':1+0.3,\n",
    "   'RBR':1+0.31,'CC':1+0.33,'EX':1+0.37,'VBP':1+0.51,'WRB':1+0.57,'PP$':1+0.59,'RB':1+0.6,'PP':1+0.78,\n",
    "   'PDT':1+0.79,'UH':1+0.83}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Processing\n",
    "Pre-processed result will have\n",
    "    words, hashtag, pos tag, negations, word emphasizes\n",
    "\n",
    "Once we get the word features, we now have to represent the tweet using them. We can have multiple variations of such representation\n",
    "1. Mean of tfidf\n",
    "2. Weighted sum of tfidf-s by hashtags, pos_tags, negations, emphasizes\n",
    "3. 2 with additional parameters\n",
    "4. Considering multiple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half Finished\n"
     ]
    }
   ],
   "source": [
    "def feature_processor1(tweetB):\n",
    "    try:\n",
    "        return vocabulary.loc[tweetB[0]].mean(axis=0)\n",
    "    except:\n",
    "        return [-999,-999]\n",
    "p_features1=np.array([feature_processor1(tweet) for tweet in p_processed])\n",
    "print(\"Half Finished\")\n",
    "n_features1=np.array([feature_processor1(tweet) for tweet in n_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35881999999999997, 0.96758999999999995]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum(p_features1[:,0]>p_features1[:,1])/100000,sum(n_features1[:,0]<n_features1[:,1])/100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half Finished\n"
     ]
    }
   ],
   "source": [
    "def pos2impact(tags):\n",
    "    return np.array([tag_scores[tag] if tag in tag_scores.keys() else 1 for tag in tags])\n",
    "def feature_processor2(tweetB):\n",
    "    try:\n",
    "        tweet=vocabulary.loc[tweetB[0]].fillna(0)\n",
    "        weights=np.array(tweetB[1])+pos2impact(tweetB[2])+tweetB[3]+tweetB[4].astype(int)\n",
    "        return [(tweet.p*weights).mean(),(tweet.n*weights).mean()]\n",
    "    except:\n",
    "        return [-999,-999]\n",
    "p_features2=np.array([feature_processor2(tweet) for tweet in p_processed])\n",
    "print(\"Half Finished\")\n",
    "n_features2=np.array([feature_processor2(tweet) for tweet in n_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36427999999999999, 0.96647000000000005]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum(p_features2[:,0]>p_features2[:,1])/100000,sum(n_features2[:,0]<n_features2[:,1])/100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_finder(tweet):\n",
    "    try:\n",
    "        return vocabulary.loc[tweet[0]].fillna(0)\n",
    "    except:\n",
    "        return [-999,-999]\n",
    "ptweet_vocs=[voc_finder(tweet) for tweet in p_processed]\n",
    "ntweet_vocs=[voc_finder(tweet) for tweet in n_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started calcuation\n",
      "Score:107925,Parameter:0\n",
      "Score:132752,Parameter:4\n"
     ]
    }
   ],
   "source": [
    "def cons_finder(tweet):\n",
    "    return [pos2impact(tweet[2]),np.array(tweet[1])+tweet[3]+tweet[4].astype(int)] #np.array(tweetB[1])+pos2impact(tweetB[2])+tweetB[3]+tweetB[4].astype(int)\n",
    "p_cons=[cons_finder(tweet) for tweet in p_processed]\n",
    "n_cons=[cons_finder(tweet) for tweet in n_processed]\n",
    "print(\"Started calcuation\")\n",
    "#Parameter estimation\n",
    "def calc(p,voc,var,cons):\n",
    "    if(type(voc)!=type([])):\n",
    "        w=p*var+cons\n",
    "        return (voc.p*w).mean()>(voc.n*w).mean()\n",
    "    else:\n",
    "        return False\n",
    "def checker(ranges,p_tweet_vocs,n_tweet_vocs):\n",
    "    for p in ranges:\n",
    "        pp=sum([calc(p,voc,cons[0],cons[1]) for voc,cons in zip(p_tweet_vocs,p_cons)])\n",
    "        nn=sum([not calc(p,voc,cons[0],cons[1]) for voc,cons in zip(n_tweet_vocs,n_cons)])\n",
    "        print(\"Score:{s},Parameter:{par}\".format(s=pp+nn,par=p))\n",
    "checker(np.arange(0,20,4),ptweet_vocs,ntweet_vocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A\n",
    "Score:133082,Parameter:0\n",
    "Score:133165,Parameter:10\n",
    "Score:133186,Parameter:20\n",
    "Score:133218,Parameter:30\n",
    "Score:133246,Parameter:40\n",
    "Score:133252,Parameter:50\n",
    "Score:133263,Parameter:60\n",
    "Score:133292,Parameter:70\n",
    "Score:133287,Parameter:80\n",
    "Score:133306,Parameter:90\n",
    "B\n",
    "Score:107925,Parameter:0\n",
    "Score:132728,Parameter:10\n",
    "Score:132714,Parameter:20\n",
    "Score:132709,Parameter:30\n",
    "Score:132710,Parameter:40\n",
    "Score:132711,Parameter:50\n",
    "Score:132711,Parameter:60\n",
    "Score:132710,Parameter:70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d=70,1,1,1\n",
    "\n",
    "def feature_processor3(tweetB):\n",
    "    tweet=vocabulary.loc[tweetB[0]].fillna(0)\n",
    "    weights=a*np.array(tweetB[1])+b*pos2impact(tweetB[2])+c*tweetB[3]+d*tweetB[4].astype(int)\n",
    "    return [(tweet.p*weights).mean(),(tweet.n*weights).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet2words(tweetB):\n",
    "    return word_scores\n",
    "def getwindow(word_scores,i,n):\n",
    "    return word_scores\n",
    "def feature_processor4(tweetB,ngram=1):\n",
    "    word_scores=tweet2words\n",
    "    total=np.array([0]*ngram)\n",
    "    for i in range(0,len(word_scores)):\n",
    "        total+=getwindow(word_scores,i,ngram)\n",
    "    return total/len(word_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
