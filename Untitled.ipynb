{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scripts.regressors as reg\n",
    "import scripts.proj1_helpers as helper\n",
    "import scripts.supportFunctions as sp\n",
    "import numpy as np\n",
    "#load the data\n",
    "y,tx,ids=helper.load_csv_data(\"data/train.csv\",standard=True,normal=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# LOSS calculators\n",
    "def sigmoid(t):\n",
    "    return 1/(1+np.exp(-t))     \n",
    "def loss_logistic(y, tx, w):\n",
    "    x=np.dot(tx,w)\n",
    "    e=np.around(sigmoid(x))-y\n",
    "    return np.mean(np.absolute(e))\n",
    "def loss_mse(y,tx,w):\n",
    "    e=y-np.dot(tx,w) #TODO: figure this out\n",
    "    return sum(e**2)/(y.shape[0])\n",
    "def loss_rmse(y,tx,w):\n",
    "    return np.sqrt(loss_mse(y,tx,w))\n",
    "def loss_abs(y,tx,w):\n",
    "    e=y-np.around(np.dot(tx,w))\n",
    "    return np.mean(np.absolute(e),axis=0)\n",
    "#GRADIENT and loss calculators\n",
    "\n",
    "#mean square error gradient\n",
    "def gradient_mse(y,tx,w):\n",
    "    rows,cols=np.indices(tx.shape)\n",
    "    tmp=y-np.sum(np.multiply(w[cols],tx),axis=1)\n",
    "    return np.sum(np.multiply(tmp[rows],tx),axis=0)/(-y.shape[0]), sum(tmp**2)/(y.shape[0])\n",
    "\n",
    "#root mean square error gradient\n",
    "def gradient_rmse(y,tx,w):\n",
    "    grad, loss=gradient_mse(y,tx,w)\n",
    "    return grad/loss, np.sqrt(loss)\n",
    "\n",
    "#stochastic mean square error gradient\n",
    "def gradient_mse_sto(y,tx,w,batch_size):\n",
    "    #getting random sample\n",
    "    target=np.random.choice(range(0,len(y)),size=batch_size)\n",
    "    #calculating the gradient\n",
    "    return gradient_mse(y[target],tx[target],w)\n",
    "#regularization parameter gradient and loss\n",
    "def regulizer(w,lambda_):\n",
    "    reg=np.absolute(w)\n",
    "    return lambda_*reg,lambda_*np.sum(reg**2)\n",
    "\n",
    "#function specific to polynomial regression\n",
    "#creating data for polynomial regression which returns all the degree of x value\n",
    "#for example if x.shape=[n,f] then it returns [n,f* ] sized matrix\n",
    "def build_poly(x, degree):\n",
    "    degree+=1\n",
    "    powers=np.ones([x.shape[0],x.shape[1]*degree])\n",
    "    for f in range(0,x.shape[1]):\n",
    "        for d in range(1,degree):\n",
    "            powers[:,f*degree+d]=x[:,f]**d\n",
    "    return powers\n",
    "\n",
    "def gradient_logistic(y, tx, w):\n",
    "    x=np.dot(tx,w)\n",
    "    tmp=sigmoid(x)-y\n",
    "    return np.dot(tx.transpose(),tmp)/y.shape[0]\n",
    "\n",
    "# Build polynomial for chosen features in the raw vector pol\n",
    "\n",
    "def build_poly_reg(x,pol, degree):\n",
    "    power=x\n",
    "    for f in range(0,len(pol)):\n",
    "    \tfor d in range(2,degree):\n",
    "\t        power=np.c_[power,x[:,pol[f]]**d]\n",
    "    return power\n",
    "\n",
    "# Obtain radial basis \n",
    "\n",
    "#Weight initializer\n",
    "def weight_init(size,lower=0,upper=1):\n",
    "    return np.random.rand(size)*(upper-lower)+lower\n",
    "\n",
    "#Logistic regression\n",
    "\n",
    "#def logistic_pdf(tx,w):\n",
    "#\tlogistic_pdf=np.ones((tx.shape[0]))/(np.ones((tx.shape[0]))+np.exp(-(np.dot(tx,w))))\n",
    "#\treturn logistic_pdf\n",
    "\n",
    "def logistic_pdf(y,tx,w):\n",
    "    logistic_pdf=np.exp(np.dot(tx,w))/(np.ones((len(y)))+np.exp(np.dot(tx,w)))\n",
    "    return logistic_pdf\n",
    "\t\n",
    "def logistic_gradient(y,tx,w):\n",
    "    log_grad=np.dot(np.transpose(tx),(y-logistic_pdf(y,tx,w)))/(len(y))\n",
    "    return log_grad\n",
    "\t\n",
    "\t\n",
    "#def logistic_gradient(y,tx,w):\n",
    " #   log_grad=np.dot(np.transpose(tx),(y-logistic_pdf(tx,w)))\n",
    "  #  return log_grad\n",
    "\n",
    "#def logistic_log_likelihood(y,tx,w):\n",
    "#\tlogisticpdf=logistic_pdf(tx,w)\n",
    "#\tlog_likelihood=np.dot(np.transpose(y),logisticpdf)+np.dot(np.transpose(np.ones((len(y)))-y),np.ones((len(y)))-logisticpdf)\n",
    "#\treturn log_likelihood\n",
    "\n",
    "def logistic_log_likelihood(y,tx,w):\n",
    "\tlogisticpdf=logistic_pdf(y,tx,w)\n",
    "\tlog_likelihood=np.dot(np.transpose(y),logisticpdf)+np.dot(np.transpose(np.ones((len(y)))-y),np.ones((len(y)))-logisticpdf)\n",
    "\treturn log_likelihood\n",
    "          \n",
    "def compute_stoch_logistic_gradient(y, x, w, batch_size):\n",
    "    index=np.int_(np.floor(len(y)*np.random.uniform(0,1,batch_size)))\n",
    "    y=y[np.transpose(index)]\n",
    "    x=x[np.transpose(index),:] \n",
    "    stoch_log_gradient=logistic_gradient(y, x, w)\n",
    "    return stoch_log_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression2(y,tx,max_iters,gamma,w_initial):\n",
    "    w=[w_initial]\n",
    "    log_likelihood=[]\n",
    "    w = w_initial\n",
    "    for n_iter in range(max_iters):\n",
    "        log_like=logistic_log_likelihood(y,tx,w)\n",
    "        grad=logistic_gradient(y,tx,w)\n",
    "        w_1=w+gamma*grad+np.random.normal(0,100,tx.shape[1])\n",
    "        log_like_1=logistic_log_likelihood(y,tx,w_1)\n",
    "        if log_like_1>log_like:\n",
    "            w=w_1\n",
    "        else :\n",
    "            w=w\n",
    "        log_likelihood.append([log_like])\n",
    "        #raise NotImplementedError\n",
    "        print(\"Gradient Descent({bi}/{ti}): like={l}\".format(\n",
    "             bi=n_iter, ti=max_iters - 1,l=log_like))\n",
    "    return  w, log_likelihood\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "(250000, 90)\n"
     ]
    }
   ],
   "source": [
    "print(\"Started\")\n",
    "degree=2\n",
    "tx=sp.build_poly(tx,degree)\n",
    "print(tx.shape)\n",
    "max_iters=2\n",
    "gamma=0.8\n",
    "w_initial=np.zeros((tx.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/1): like=125000.0\n",
      "Gradient Descent(1/1): like=125000.0\n",
      "(array([ 246.88661156,  -84.96254924,  -17.25602838,   25.72608738,\n",
      "         -9.57854478,  -61.78376948, -186.55636808,    4.22562927,\n",
      "        150.25915957,  -17.31496691,   19.07083251,    5.52987402,\n",
      "        -64.41703101,  155.86005866,   14.80862714,   97.81667642,\n",
      "        157.96110955, -125.01167381,   33.46131941,    7.03825787,\n",
      "         31.4472125 ,  128.81260964,  -23.99007085,  -66.00414315,\n",
      "        -27.51415226, -155.0655097 ,  -24.45399674, -218.41575858,\n",
      "        -63.71646168,  128.76099982,  126.57304933,  -18.6690838 ,\n",
      "         -3.9542886 ,   40.02948413,  120.44762928, -111.00949098,\n",
      "         61.34748774,  -33.77671701,   56.09949902,  126.95152935,\n",
      "         89.24502454,  123.73667205,   78.7430585 ,   25.47244272,\n",
      "        -42.29745662,  -18.29420991, -117.35200935,  167.63943647,\n",
      "         27.43239361,   77.19713592,  -14.45334486,  -26.41452497,\n",
      "         28.60422019,  -63.02567412,  127.63764954, -166.22659822,\n",
      "        -24.51397157,   35.05450015,  -35.02910848,   -5.29782848,\n",
      "        -44.14033455,   16.5774202 , -132.47860168,    8.18268741,\n",
      "         26.13681868,  -71.30680375,  -44.10792035,  -52.49370122,\n",
      "         10.98675199,  -71.83371311,  125.32215656,  223.67360756,\n",
      "         10.88483519,   17.9202898 , -187.44847573,  -41.3631796 ,\n",
      "        -99.9890244 ,   22.291913  ,  -18.45639513,   47.2253966 ,\n",
      "        148.33255778,  189.48022854,   96.68056462,  164.76559359,\n",
      "        -38.23392466,   78.37955647,   56.24866218, -192.66562688,\n",
      "         10.87728482,  -15.98967031]), [[125000.0], [125000.0]])\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "w=logistic_regression2(y,tx,max_iters,gamma,w_initial)\n",
    "print(w)\n",
    "#print(w.shape)\n",
    "w=np.asarray(w)\n",
    "print(w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
