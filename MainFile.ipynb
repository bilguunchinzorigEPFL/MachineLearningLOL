{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 1,
   "metadata": {},
>>>>>>> b75765da9cca20c4ddf8311d6f45f84c18cb48d3
   "outputs": [],
   "source": [
    "import scripts.regressors as reg\n",
    "import scripts.proj1_helpers as helper\n",
    "import scripts.supportFunctions as sp\n",
    "import pandas as pd\n",
    "#load the data\n",
    "y,tx,ids=helper.load_csv_data(\"data/train.csv\",standard=True)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): like=88366.61391385623\n",
      "Gradient Descent(1/99): like=103735.09214009412\n",
      "Gradient Descent(2/99): like=117112.14610709058\n",
      "Gradient Descent(3/99): like=105847.82448315396\n",
      "Gradient Descent(4/99): like=110729.84784257521\n",
      "Gradient Descent(5/99): like=96828.88671853532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moka/Dropbox/1.PHD/2.COURSESTAKEN/MACHINELEARNING/Project1/MachineLearningLOL/scripts/supportFunctions.py:65: RuntimeWarning: overflow encountered in exp\n",
      "  logistic_pdf=np.ones((len(y)))/(np.ones((len(y)))+np.exp(-np.dot(tx,w)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6/99): like=90474.57739528886\n",
      "Gradient Descent(7/99): like=97086.52597044781\n",
      "Gradient Descent(8/99): like=91192.75828091709\n",
      "Gradient Descent(9/99): like=89752.87505409673\n",
      "Gradient Descent(10/99): like=85381.21046224172\n",
      "Gradient Descent(11/99): like=84836.59494849703\n",
      "Gradient Descent(12/99): like=85295.92922284195\n",
      "Gradient Descent(13/99): like=86736.17411454748\n",
      "Gradient Descent(14/99): like=87875.29618185098\n",
      "Gradient Descent(15/99): like=88344.34239321544\n",
      "Gradient Descent(16/99): like=90369.75047078589\n",
      "Gradient Descent(17/99): like=94969.79272877617\n",
      "Gradient Descent(18/99): like=90969.67081046103\n",
      "Gradient Descent(19/99): like=90235.38897740949\n",
      "Gradient Descent(20/99): like=87098.89916163648\n",
      "Gradient Descent(21/99): like=89147.9195664608\n",
      "Gradient Descent(22/99): like=93315.92385783482\n",
      "Gradient Descent(23/99): like=98770.47325488541\n",
      "Gradient Descent(24/99): like=93405.29128186769\n",
      "Gradient Descent(25/99): like=91904.8669353385\n",
      "Gradient Descent(26/99): like=96336.16315289169\n",
      "Gradient Descent(27/99): like=92437.07423848278\n",
      "Gradient Descent(28/99): like=91610.85586567651\n",
      "Gradient Descent(29/99): like=91298.9077488642\n",
      "Gradient Descent(30/99): like=93102.16155133821\n",
      "Gradient Descent(31/99): like=91123.37199103535\n",
      "Gradient Descent(32/99): like=93019.60236758649\n",
      "Gradient Descent(33/99): like=92184.86486672785\n",
      "Gradient Descent(34/99): like=93389.29913000546\n",
      "Gradient Descent(35/99): like=92119.16095082389\n",
      "Gradient Descent(36/99): like=87295.26994217042\n",
      "Gradient Descent(37/99): like=88699.83446593765\n",
      "Gradient Descent(38/99): like=85258.55444804221\n",
      "Gradient Descent(39/99): like=82947.12458082169\n",
      "Gradient Descent(40/99): like=80633.15278016301\n",
      "Gradient Descent(41/99): like=79953.85491118622\n",
      "Gradient Descent(42/99): like=79710.27364069116\n",
      "Gradient Descent(43/99): like=78880.536064046\n",
      "Gradient Descent(44/99): like=79200.0547854588\n",
      "Gradient Descent(45/99): like=78613.46597539289\n",
      "Gradient Descent(46/99): like=78624.71466288819\n",
      "Gradient Descent(47/99): like=75878.94549472578\n",
      "Gradient Descent(48/99): like=75837.88223778893\n",
      "Gradient Descent(49/99): like=77683.00407640942\n",
      "Gradient Descent(50/99): like=78083.87766541696\n",
      "Gradient Descent(51/99): like=78768.35995020074\n",
      "Gradient Descent(52/99): like=79590.08539162147\n",
      "Gradient Descent(53/99): like=81929.55285698015\n",
      "Gradient Descent(54/99): like=82482.13963991855\n",
      "Gradient Descent(55/99): like=83060.85657947167\n",
      "Gradient Descent(56/99): like=83805.55040390907\n",
      "Gradient Descent(57/99): like=87421.08158631128\n",
      "Gradient Descent(58/99): like=88015.21028635338\n",
      "Gradient Descent(59/99): like=88166.82930981484\n",
      "Gradient Descent(60/99): like=87769.73580114043\n",
      "Gradient Descent(61/99): like=86673.7625013052\n",
      "Gradient Descent(62/99): like=90775.93606772038\n",
      "Gradient Descent(63/99): like=88398.56322196474\n",
      "Gradient Descent(64/99): like=87988.51825090572\n",
      "Gradient Descent(65/99): like=88827.93995859277\n",
      "Gradient Descent(66/99): like=89959.52678301674\n",
      "Gradient Descent(67/99): like=89805.11683463899\n",
      "Gradient Descent(68/99): like=88277.65916029584\n",
      "Gradient Descent(69/99): like=85588.37785148717\n",
      "Gradient Descent(70/99): like=87529.39474889747\n",
      "Gradient Descent(71/99): like=89507.53212944535\n",
      "Gradient Descent(72/99): like=89824.64491551985\n",
      "Gradient Descent(73/99): like=88854.18117780887\n",
      "Gradient Descent(74/99): like=87854.6526575953\n",
      "Gradient Descent(75/99): like=90561.11073553705\n",
      "Gradient Descent(76/99): like=90101.35251246326\n",
      "Gradient Descent(77/99): like=92585.68724874745\n",
      "Gradient Descent(78/99): like=89734.45429283625\n",
      "Gradient Descent(79/99): like=88425.25907092009\n",
      "Gradient Descent(80/99): like=88964.71507992655\n",
      "Gradient Descent(81/99): like=90053.52515970309\n",
      "Gradient Descent(82/99): like=88828.97790995616\n",
      "Gradient Descent(83/99): like=89406.39213449668\n",
      "Gradient Descent(84/99): like=88093.85254055155\n",
      "Gradient Descent(85/99): like=88494.06170352941\n",
      "Gradient Descent(86/99): like=87437.51762524832\n",
      "Gradient Descent(87/99): like=87282.85604442643\n",
      "Gradient Descent(88/99): like=86615.39410310496\n",
      "Gradient Descent(89/99): like=87559.48014819871\n",
      "Gradient Descent(90/99): like=88601.49954745325\n",
      "Gradient Descent(91/99): like=89387.6624198939\n",
      "Gradient Descent(92/99): like=88879.28485952545\n",
      "Gradient Descent(93/99): like=89613.70295230401\n",
      "Gradient Descent(94/99): like=90112.78773096889\n",
      "Gradient Descent(95/99): like=89806.24664132045\n",
      "Gradient Descent(96/99): like=89910.78112793363\n",
      "Gradient Descent(97/99): like=89299.94401133282\n",
      "Gradient Descent(98/99): like=88046.40732624003\n",
      "Gradient Descent(99/99): like=86877.98429045154\n",
      "[[88366.613913856228], [103735.09214009412], [117112.14610709058], [105847.82448315396], [110729.84784257521], [96828.886718535316], [90474.577395288856], [97086.525970447809], [91192.758280917085], [89752.87505409673], [85381.210462241725], [84836.594948497033], [85295.929222841951], [86736.174114547481], [87875.296181850979], [88344.34239321544], [90369.75047078589], [94969.79272877617], [90969.67081046103], [90235.388977409486], [87098.89916163648], [89147.919566460798], [93315.923857834816], [98770.473254885408], [93405.291281867685], [91904.866935338505], [96336.163152891691], [92437.074238482775], [91610.855865676509], [91298.907748864207], [93102.161551338213], [91123.371991035354], [93019.602367586485], [92184.864866727847], [93389.299130005456], [92119.160950823891], [87295.269942170416], [88699.834465937645], [85258.55444804221], [82947.124580821692], [80633.152780163015], [79953.854911186223], [79710.273640691157], [78880.536064045999], [79200.054785458793], [78613.465975392886], [78624.714662888189], [75878.945494725776], [75837.88223778893], [77683.004076409416], [78083.877665416963], [78768.359950200742], [79590.085391621469], [81929.552856980154], [82482.139639918547], [83060.856579471671], [83805.550403909074], [87421.08158631128], [88015.21028635338], [88166.829309814842], [87769.735801140429], [86673.762501305202], [90775.93606772038], [88398.563221964738], [87988.518250905719], [88827.939958592775], [89959.526783016743], [89805.11683463899], [88277.659160295836], [85588.377851487166], [87529.394748897466], [89507.532129445346], [89824.64491551985], [88854.181177808874], [87854.652657595303], [90561.110735537048], [90101.352512463258], [92585.687248747447], [89734.45429283625], [88425.259070920089], [88964.715079926551], [90053.525159703087], [88828.977909956157], [89406.39213449668], [88093.852540551554], [88494.061703529413], [87437.517625248322], [87282.856044426429], [86615.394103104962], [87559.480148198709], [88601.499547453248], [89387.662419893895], [88879.284859525447], [89613.702952304011], [90112.787730968892], [89806.246641320453], [89910.781127933631], [89299.944011332816], [88046.407326240034], [86877.984290451539]] 36616896.0509\n"
     ]
    }
   ],
   "source": [
    "tr_idx,te_idx=helper.split_data(y,0.8)\n",
    "# Train the regressor\n",
    "# 1. Least squares \n",
    "#w,tr_loss=reg.least_squares(y[tr_idx],tx[tr_idx])\n",
    "# 2. Logistic regression\n",
    "max_iters=100\n",
    "gamma=0.8\n",
    "#w_initial = np.zeros((30))\n",
    "w_initial = np.random.rand(tx.shape[1])\n",
    "w,tr_loss=reg.logistic_regression(y[tr_idx],tx[tr_idx],max_iters,gamma,w_initial)\n",
    "#evaluate the result\n",
    "te_loss=sp.loss_mse(y[te_idx],tx[te_idx],w)\n",
    "print(tr_loss,te_loss)\n",
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(data=tx)\n",
    "correlation=data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -0.171892\n",
       "1    -0.419757\n",
       "2    -0.090846\n",
       "3    -0.539379\n",
       "4    -0.679068\n",
       "5    -0.706110\n",
       "6    -0.706110\n",
       "7    -0.539379\n",
       "8    -0.148081\n",
       "9    -0.432603\n",
       "10   -0.474633\n",
       "11   -0.419757\n",
       "12   -0.527090\n",
       "13   -0.474633\n",
       "14   -0.003330\n",
       "15   -0.207026\n",
       "16   -0.069957\n",
       "17   -0.006777\n",
       "18   -0.222633\n",
       "19   -0.402345\n",
       "20   -0.225064\n",
       "21   -0.407002\n",
       "22   -0.347904\n",
       "23   -0.480736\n",
       "24   -0.157561\n",
       "25   -0.225064\n",
       "26   -0.318731\n",
       "27   -0.157561\n",
       "28   -0.075504\n",
       "29   -0.448737\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation[correlation==1]=0\n",
    "correlation.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12718307,  0.08060575,  0.07214969, ...,  1.27458201,\n",
       "         0.21225415,  0.07273731],\n",
       "       [ 0.14781875,  0.10730996,  0.0764141 , ...,  0.99923677,\n",
       "         0.99985389,  0.02962506],\n",
       "       [ 0.        ,  0.25306351,  0.09432812, ...,  0.99923677,\n",
       "         0.99985389,  0.02835933],\n",
       "       ..., \n",
       "       [ 0.09686102,  0.09444862,  0.05481131, ...,  0.99923677,\n",
       "         0.99985389,  0.02691159],\n",
       "       [ 0.08721138,  0.0302137 ,  0.04927025, ...,  0.99923677,\n",
       "         0.99985389,  0.        ],\n",
       "       [ 0.        ,  0.11353309,  0.05086231, ...,  0.99923677,\n",
       "         0.99985389,  0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize\n",
    "import numpy as np\n",
    "centered_data = tx - np.mean(tx, axis=0)\n",
    "std_data = centered_data / np.std(centered_data, axis=0)\n",
    "normalized=(std_data-std_data.min(axis=0))/std_data.max(axis=0)\n",
    "normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"data/train.csv\"\n",
    "y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "ids = x[:, 0].astype(np.int)\n",
    "input_data = x[:, 2:]\n",
>>>>>>> b75765da9cca20c4ddf8311d6f45f84c18cb48d3
    "\n",
    "# convert class labels from strings to binary (1,0).We assign 1 for label \"b\" and 0 otherwise\n",
    "yb = np.zeros(len(y))\n",
    "yb[np.where(y=='b')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.38470000e+02,   5.16550000e+01,   9.78270000e+01, ...,\n",
       "          1.24000000e+00,  -2.47500000e+00,   1.13497000e+02],\n",
       "       [  1.60937000e+02,   6.87680000e+01,   1.03235000e+02, ...,\n",
       "         -1.18452642e-02,  -1.58228913e-03,   4.62260000e+01],\n",
       "       [  1.21858528e+02,   1.62172000e+02,   1.25953000e+02, ...,\n",
       "         -1.18452642e-02,  -1.58228913e-03,   4.42510000e+01],\n",
       "       ..., \n",
       "       [  1.05457000e+02,   6.05260000e+01,   7.58390000e+01, ...,\n",
       "         -1.18452642e-02,  -1.58228913e-03,   4.19920000e+01],\n",
       "       [  9.49510000e+01,   1.93620000e+01,   6.88120000e+01, ...,\n",
       "         -1.18452642e-02,  -1.58228913e-03,   0.00000000e+00],\n",
       "       [  1.21858528e+02,   7.27560000e+01,   7.08310000e+01, ...,\n",
       "         -1.18452642e-02,  -1.58228913e-03,   0.00000000e+00]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def null_replacer(data,null_val= -999):\n",
    "    x=data.copy()\n",
    "    for c in range(0,x.shape[1]):\n",
    "        total=0\n",
    "        num=0\n",
    "        nulls=[]\n",
    "        for r in range(0,x.shape[0]):\n",
    "            val=x[r,c]\n",
    "            if val!=null_val:\n",
    "                num+=1\n",
    "                total+=val\n",
    "            else: nulls.append(r)\n",
    "        x[nulls,c]=total/num\n",
    "    return x\n",
    "null_replacer(input_data)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
=======
   "execution_count": null,
>>>>>>> b75765da9cca20c4ddf8311d6f45f84c18cb48d3
   "metadata": {},
   "outputs": [],
   "source": [
    "w_initial = np.random.rand(tx.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.5909894484039182\n",
      "Current iteration=1, loss=0.587520430514015\n",
      "Current iteration=2, loss=0.5830840903278889\n",
      "Current iteration=3, loss=0.5789590722600874\n",
      "Current iteration=4, loss=0.5744449015443801\n",
      "Current iteration=5, loss=0.5681517472953891\n",
      "Current iteration=6, loss=0.5614694403984923\n",
      "Current iteration=7, loss=0.5542312011474444\n",
      "Current iteration=8, loss=0.546281368483083\n",
      "Current iteration=9, loss=0.5378089594058195\n",
      "Current iteration=10, loss=0.5294810927406354\n",
      "Current iteration=11, loss=0.5209530904279567\n",
      "Current iteration=12, loss=0.5119025117023761\n",
      "Current iteration=13, loss=0.5040749841559279\n",
      "Current iteration=14, loss=0.4954580326665851\n",
      "Current iteration=15, loss=0.487630505120137\n",
      "Current iteration=16, loss=0.47941382492578305\n",
      "Current iteration=17, loss=0.4719754500272407\n",
      "Current iteration=18, loss=0.46557110930741946\n",
      "Current iteration=19, loss=0.4594002601763417\n",
      "Current iteration=20, loss=0.4534295466927584\n",
      "Current iteration=21, loss=0.4485151046820623\n",
      "Current iteration=22, loss=0.4426666963163922\n",
      "Current iteration=23, loss=0.43758547459945074\n",
      "Current iteration=24, loss=0.43289340553041505\n",
      "Current iteration=25, loss=0.42895740446302494\n",
      "Current iteration=26, loss=0.42435428457065344\n",
      "Current iteration=27, loss=0.42002913085535754\n",
      "Current iteration=28, loss=0.4161709603175486\n",
      "Current iteration=29, loss=0.41250180678015097\n",
      "Current iteration=30, loss=0.40875482271317226\n",
      "Current iteration=31, loss=0.4053191607645182\n",
      "Current iteration=32, loss=0.40243943117001524\n",
      "Current iteration=33, loss=0.3987035657501195\n",
      "Current iteration=34, loss=0.39519007327188427\n",
      "Current iteration=35, loss=0.39187671644114347\n",
      "Current iteration=36, loss=0.3886745460812328\n",
      "Current iteration=37, loss=0.38582817242797895\n",
      "Current iteration=38, loss=0.38292620553930995\n",
      "Current iteration=39, loss=0.38052457776937704\n",
      "Current iteration=40, loss=0.3783230856469385\n",
      "Current iteration=41, loss=0.3755767798174318\n",
      "Current iteration=42, loss=0.373230745282914\n",
      "Current iteration=43, loss=0.3709625412779773\n",
      "Current iteration=44, loss=0.36879440509678785\n",
      "Current iteration=45, loss=0.3667485740335116\n",
      "Current iteration=46, loss=0.36443589544024285\n",
      "Current iteration=47, loss=0.3623344711415515\n",
      "Current iteration=48, loss=0.36053325031410177\n",
      "Current iteration=49, loss=0.3592434872524711\n",
      "Current iteration=50, loss=0.3574867410133535\n",
      "Current iteration=51, loss=0.3559412490688133\n",
      "Current iteration=52, loss=0.3544179944184392\n",
      "Current iteration=53, loss=0.352861383826816\n",
      "Current iteration=54, loss=0.35162721400060043\n",
      "Current iteration=55, loss=0.3503596882331358\n",
      "Current iteration=56, loss=0.34915887434816933\n",
      "Current iteration=57, loss=0.34805812828695004\n",
      "Current iteration=58, loss=0.3463013820478324\n",
      "Current iteration=59, loss=0.3452228732807792\n",
      "Current iteration=60, loss=0.3441110085724769\n",
      "Current iteration=61, loss=0.3430992116879218\n",
      "Current iteration=62, loss=0.34184280456754024\n",
      "Current iteration=63, loss=0.34083100768298513\n",
      "Current iteration=64, loss=0.3395412446213545\n",
      "Current iteration=65, loss=0.33860727826638054\n",
      "Current iteration=66, loss=0.3374620576168292\n",
      "Current iteration=67, loss=0.3370395490276743\n",
      "Current iteration=68, loss=0.335916565672289\n",
      "Current iteration=69, loss=0.33487141284648486\n",
      "Current iteration=70, loss=0.3336817176086014\n",
      "Current iteration=71, loss=0.3328700563715407\n",
      "Current iteration=72, loss=0.332036157840314\n",
      "Current iteration=73, loss=0.3313023271328345\n",
      "Current iteration=74, loss=0.33037947942494356\n",
      "Current iteration=75, loss=0.32959005548204895\n",
      "Current iteration=76, loss=0.3288451061274864\n",
      "Current iteration=77, loss=0.32834476700875037\n",
      "Current iteration=78, loss=0.32759981765418783\n",
      "Current iteration=79, loss=0.3267436818287951\n",
      "Current iteration=80, loss=0.32629893594547416\n",
      "Current iteration=81, loss=0.32562069847340974\n",
      "Current iteration=82, loss=0.3248090372363491\n",
      "Current iteration=83, loss=0.3242308675880319\n",
      "Current iteration=84, loss=0.32370829117512984\n",
      "Current iteration=85, loss=0.3229188672322352\n",
      "Current iteration=86, loss=0.32221839246600475\n",
      "Current iteration=87, loss=0.3215290363468573\n",
      "Current iteration=88, loss=0.3209397480514571\n",
      "Current iteration=89, loss=0.3206061886389664\n",
      "Current iteration=90, loss=0.3201836800498115\n",
      "Current iteration=91, loss=0.31964998498982644\n",
      "Current iteration=92, loss=0.31921635775358853\n",
      "Current iteration=93, loss=0.31888279834109784\n",
      "Current iteration=94, loss=0.3185603575756902\n",
      "Current iteration=95, loss=0.3182712727515316\n",
      "Current iteration=96, loss=0.317993306574456\n",
      "Current iteration=97, loss=0.31742625557322185\n",
      "Current iteration=98, loss=0.31675913674824047\n",
      "Current iteration=99, loss=0.3161364925115912\n",
      "Current iteration=100, loss=0.31565839068702123\n",
      "Current iteration=101, loss=0.31532483127453054\n",
      "Current iteration=102, loss=0.31514693292120216\n",
      "Current iteration=103, loss=0.31431303438997543\n",
      "Current iteration=104, loss=0.3138460512124885\n",
      "Current iteration=105, loss=0.3135347290941638\n",
      "Current iteration=106, loss=0.31321228832875614\n",
      "Current iteration=107, loss=0.3125340508566918\n",
      "Current iteration=108, loss=0.3119336439142085\n",
      "Current iteration=109, loss=0.3115333726192197\n",
      "Current iteration=110, loss=0.3111108640300648\n",
      "Current iteration=111, loss=0.31059940626424576\n",
      "Current iteration=112, loss=0.3101880163221739\n",
      "Current iteration=113, loss=0.3097877450271851\n",
      "Current iteration=114, loss=0.30938747373219627\n",
      "Current iteration=115, loss=0.3090205583784565\n",
      "Current iteration=116, loss=0.30874259220138095\n",
      "Current iteration=117, loss=0.3084757446713884\n",
      "Current iteration=118, loss=0.3080865920234826\n",
      "Current iteration=119, loss=0.3075973715518296\n",
      "Current iteration=120, loss=0.30730828672767097\n",
      "Current iteration=121, loss=0.3070747951389275\n",
      "Current iteration=122, loss=0.3067745916676859\n",
      "Current iteration=123, loss=0.3066745238439387\n",
      "Current iteration=124, loss=0.3060741169014554\n",
      "Current iteration=125, loss=0.30556265913563635\n",
      "Current iteration=126, loss=0.3051401505464815\n",
      "Current iteration=127, loss=0.304884421663572\n",
      "Current iteration=128, loss=0.3048510657223229\n",
      "Current iteration=129, loss=0.3046620487219115\n",
      "Current iteration=130, loss=0.3042617774269227\n",
      "Current iteration=131, loss=0.303928218014432\n",
      "Current iteration=132, loss=0.30377255695526967\n",
      "Current iteration=133, loss=0.303450116189862\n",
      "Current iteration=134, loss=0.30316103136570344\n",
      "Current iteration=135, loss=0.3027718787177976\n",
      "Current iteration=136, loss=0.30263845495280134\n",
      "Current iteration=137, loss=0.30239384471697484\n",
      "Current iteration=138, loss=0.3021381158340653\n",
      "Current iteration=139, loss=0.30167113265657836\n",
      "Current iteration=140, loss=0.30138204783241973\n",
      "Current iteration=141, loss=0.30109296300826116\n",
      "Current iteration=142, loss=0.3008928273607667\n",
      "Current iteration=143, loss=0.30050367471286094\n",
      "Current iteration=144, loss=0.3002479458299514\n",
      "Current iteration=145, loss=0.29996997965287586\n",
      "Current iteration=146, loss=0.299547471063721\n",
      "Current iteration=147, loss=0.2992139116512303\n",
      "Current iteration=148, loss=0.2989470641212377\n",
      "Current iteration=149, loss=0.298591267414581\n",
      "Current iteration=150, loss=0.2982465893550073\n",
      "Current iteration=151, loss=0.2979019112954336\n",
      "Current iteration=152, loss=0.29780184347168637\n",
      "Current iteration=153, loss=0.297635063765441\n",
      "Current iteration=154, loss=0.29724591111753523\n",
      "Current iteration=155, loss=0.29692347035212757\n",
      "Current iteration=156, loss=0.2966121482338029\n",
      "Current iteration=157, loss=0.2962897074683952\n",
      "Current iteration=158, loss=0.2962897074683952\n"
     ]
    }
   ],
   "source": [
    "w=reg.logistic_regression_gradient_descent_demo(y[tr_idx],tx[tr_idx])\n",
    "\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.23486944e-01   5.95335966e-01   8.70925226e-01  -1.56848113e-01\n",
      "   5.30827324e-01  -1.37367605e+00  -3.04577637e-02  -7.75833649e-01\n",
      "   3.49835863e-02   3.20185438e-01   4.85920868e-01  -2.20050870e-01\n",
      "  -2.80062185e-01  -6.43579542e-01   1.01551994e-02   2.42133981e-03\n",
      "  -8.24078082e-01   8.89688917e-03  -5.04723543e-03  -2.45691737e-01\n",
      "   3.21834538e-03   1.62992666e-01  -1.69873790e-01  -3.75298436e-01\n",
      "   1.36597610e-03   4.68028830e-03   1.03005597e-01  -1.08552651e-02\n",
      "  -2.27717784e-03   7.10646762e-01]\n",
      "156373.266214\n"
     ]
    }
   ],
   "source": [
    "def logistic_pdf(y,tx,w):\n",
    "    logistic_pdf=np.ones((len(y)))/(np.ones((len(y)))+np.exp(-np.dot(tx,w)))\n",
    "    return logistic_pdf\n",
    "\n",
    "def logistic_log_likelihood(y,tx,w):\n",
    "    logisticpdf=logistic_pdf(y,tx,w)\n",
    "    log_likelihood=np.dot(np.transpose(y),logisticpdf)+np.dot(np.transpose(np.ones((len(y)))-y),np.ones((len(y)))-logisticpdf)\n",
    "    return log_likelihood\n",
    "print(w)\n",
    "log=logistic_log_likelihood(y,tx,w)\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4974.9790000000003"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138.47"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_likelihood(y,tx,w):\n",
    "    logisticpdf=logistic_pdf(y,tx,w)\n",
    "    log_likelihood=np.dot(np.transpose(y),logisticpdf)+np.dot(np.transpose(np.ones((len(y)))-y),np.ones((len(y)))-logisticpdf)\n",
    "    return log_likelihood\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
